{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files were selected for the category 02691156 : 4045 / 8089\n",
      "files were selected for the category 02828884 : 1814 / 3627\n",
      "files were selected for the category 02933112 : 1572 / 3143\n",
      "files were selected for the category 02958343 : 3515 / 7029\n",
      "files were selected for the category 03001627 : 6778 / 13556\n",
      "files were selected for the category 03211117 : 1094 / 2187\n",
      "files were selected for the category 03636649 : 2310 / 4619\n",
      "files were selected for the category 03691459 : 1598 / 3195\n",
      "files were selected for the category 04090263 : 2374 / 4747\n",
      "files were selected for the category 04256520 : 3174 / 6347\n",
      "files were selected for the category 04379243 : 8436 / 16872\n",
      "files were selected for the category 04401088 : 1090 / 2179\n",
      "files were selected for the category 04530566 : 1940 / 3879\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "class AverageValueMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "CHUNK_SIZE  = 150\n",
    "lenght_line = 60\n",
    "def my_get_n_random_lines(path, n=5):\n",
    "    MY_CHUNK_SIZE = lenght_line * (n+2)\n",
    "    lenght = os.stat(path).st_size\n",
    "    with open(path, 'r') as file:\n",
    "            file.seek(random.randint(400, lenght - MY_CHUNK_SIZE))\n",
    "            chunk = file.read(MY_CHUNK_SIZE)\n",
    "            lines = chunk.split(os.linesep)\n",
    "            return lines[1:n+1]\n",
    "\n",
    "\n",
    "datapath = []\n",
    "cat      = {}\n",
    "meta     = {}\n",
    "catfile  = os.path.join('../data/synsetoffset2category.txt')\n",
    "rootpc   = \"/home/tdeprelle/ssd/data/customShapeNet/\"\n",
    "\n",
    "with open(catfile, 'r') as f:\n",
    "    for line in f:\n",
    "        ls = line.strip().split()\n",
    "        cat[ls[0]] = ls[1]\n",
    "        \n",
    "empty = []\n",
    "for item in cat:\n",
    "    dir_img  = os.path.join(rootpc, cat[item]) + \"/ply/\"\n",
    "    fns_img = sorted(os.listdir(dir_img))\n",
    "    \n",
    "    fns_pc = []\n",
    "    fns = [val for val in fns_img if val[-4:]==\".ply\"]\n",
    "\n",
    "    print(\"files were selected for the category\", cat[item], ':', str(len(fns)), '/', len(fns_img))\n",
    "\n",
    "\n",
    "    if len(fns) != 0:\n",
    "        meta[item] = []\n",
    "        for fn in fns:\n",
    "            meta[item].append( ( os.path.join(dir_img, fn, \"rendering\"), os.path.join(dir_point, fn ), item, fn ) )\n",
    "    else:\n",
    "        empty.append(item)   \n",
    "        \n",
    "idx2cat = {}\n",
    "size    = {}\n",
    "i       = 0       \n",
    "\n",
    "for item in cat:\n",
    "    idx2cat[i] = item\n",
    "    size[i] = len(meta[item])\n",
    "    i = i + 1\n",
    "    for fn in meta[item]:\n",
    "        datapath.append(fn)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='/home/tdeprelle/ssd/data/customShapeNet/04530566/ply/ffffe224db39febe288b05b36358465d.points.ply' mode='r' encoding='UTF-8'>\n",
      "MY_CHUNK_SIZE 30120\n",
      "lenght 1686112\n",
      "3000\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "fn1 = datapath[i]\n",
    "\n",
    "with open(fn[1]) as fp:\n",
    "    print(fp)\n",
    "    for i, line in enumerate(fp):\n",
    "        if i == 2:\n",
    "            try:\n",
    "                lenght = int(line.split()[2])\n",
    "            except ValueError:\n",
    "                print(fn)\n",
    "                print(line)\n",
    "            break\n",
    "\n",
    "npoints = 500\n",
    "for i in range(15):\n",
    "    try:\n",
    "        mystring = my_get_n_random_lines(fn[1], n = npoints)\n",
    "        point_set = np.loadtxt(mystring).astype(np.float32)\n",
    "        print(point_set.size)\n",
    "        break\n",
    "    except ValueError as excep:\n",
    "        print(fn)\n",
    "        print(excep)\n",
    "        \n",
    "print(point_set.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch.utils.data as data\n",
    "import os.path\n",
    "import errno\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "from utils import *\n",
    "\n",
    "\n",
    "class ShapeNet(data.Dataset):\n",
    "    def __init__(self, rootimg = \"/home/tdeprelle/ssd/data/customShapeNet/\", rootpc = \"/home/tdeprelle/ssd/data/customShapeNet/\" , method = 'PMA', class_choice = \"chair\", train = True, npoints = 2500, normal = False, balanced = False, gen_view=False, SVR=False, idx=0):\n",
    "        self.balanced = balanced\n",
    "        self.normal = normal\n",
    "        self.train = train\n",
    "        self.method = method\n",
    "        self.rootimg = rootimg\n",
    "        self.rootpc = rootpc\n",
    "        self.npoints = npoints\n",
    "        self.datapath = []\n",
    "        self.catfile = os.path.join('./data/synsetoffset2category.txt')\n",
    "        self.cat = {}\n",
    "        self.meta = {}\n",
    "        self.SVR = SVR\n",
    "        self.gen_view = gen_view\n",
    "        self.idx=idx\n",
    "        with open(self.catfile, 'r') as f:\n",
    "            for line in f:\n",
    "                ls = line.strip().split()\n",
    "                self.cat[ls[0]] = ls[1]\n",
    "        if not class_choice is  None:\n",
    "            self.cat = {k:v for k,v in self.cat.items() if k in class_choice}\n",
    "        print(self.cat)\n",
    "        empty = []\n",
    "        for item in self.cat:\n",
    "            dir_img  = os.path.join(self.rootimg, self.cat[item])\n",
    "            fns_img = sorted(os.listdir(dir_img))\n",
    "\n",
    "            try:\n",
    "                dir_point = os.path.join(self.rootpc, self.cat[item], 'ply')\n",
    "                fns_pc = sorted(os.listdir(dir_point))\n",
    "            except:\n",
    "                fns_pc = []\n",
    "            fns = [val for val in fns_img if val + '.points.ply' in fns_pc]\n",
    "            print('category ', self.cat[item], 'files ' + str(len(fns)), len(fns)/float(len(fns_img)), \"%\"),\n",
    "            if train:\n",
    "                fns = fns[:int(len(fns) * 0.8)]\n",
    "            else:\n",
    "                fns = fns[int(len(fns) * 0.8):]\n",
    "\n",
    "\n",
    "            if len(fns) != 0:\n",
    "                self.meta[item] = []\n",
    "                for fn in fns:\n",
    "                    objpath = \"/home/thibault/Downloads/data/ssd/ShapeNetCorev2/\" +  self.cat[item] + \"/\" + fn + \"/models/model_normalized.ply\"\n",
    "                    self.meta[item].append( ( os.path.join(dir_img, fn, \"rendering\"), os.path.join(dir_point, fn + '.points.ply'), item, objpath, fn ) )\n",
    "            else:\n",
    "                empty.append(item)\n",
    "        for item in empty:\n",
    "            del self.cat[item]\n",
    "        self.idx2cat = {}\n",
    "        self.size = {}\n",
    "        i = 0\n",
    "        for item in self.cat:\n",
    "            self.idx2cat[i] = item\n",
    "            self.size[i] = len(self.meta[item])\n",
    "            i = i + 1\n",
    "            for fn in self.meta[item]:\n",
    "                self.datapath.append(fn)\n",
    "\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        self.transforms = transforms.Compose([\n",
    "                             transforms.Scale(size =  224, interpolation = 2),\n",
    "                             transforms.ToTensor(),\n",
    "                             # normalize,\n",
    "                        ])\n",
    "\n",
    "        # RandomResizedCrop or RandomCrop\n",
    "        self.dataAugmentation = transforms.Compose([\n",
    "                                         transforms.RandomCrop(127),\n",
    "                                         transforms.RandomHorizontalFlip(),\n",
    "                            ])\n",
    "        self.validating = transforms.Compose([\n",
    "                        transforms.CenterCrop(127),\n",
    "                        ])\n",
    "\n",
    "        self.perCatValueMeter = {}\n",
    "        for item in self.cat:\n",
    "            self.perCatValueMeter[item] = AverageValueMeter()\n",
    "        self.perCatValueMeter_metro = {}\n",
    "        for item in self.cat:\n",
    "            self.perCatValueMeter_metro[item] = AverageValueMeter()\n",
    "        self.transformsb = transforms.Compose([\n",
    "                             transforms.Scale(size =  224, interpolation = 2),\n",
    "                        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fn = self.datapath[index]\n",
    "        with open(fn[1]) as fp:\n",
    "            for i, line in enumerate(fp):\n",
    "                if i == 2:\n",
    "                    try:\n",
    "                        lenght = int(line.split()[2])\n",
    "                    except ValueError:\n",
    "                        print(fn)\n",
    "                        print(line)\n",
    "                    break\n",
    "        for i in range(15):\n",
    "            try:\n",
    "                mystring = my_get_n_random_lines(fn[1], n = self.npoints)\n",
    "                point_set = np.loadtxt(mystring).astype(np.float32)\n",
    "                break\n",
    "            except ValueError as excep:\n",
    "                print(fn)\n",
    "                print(excep)\n",
    "\n",
    "        # centroid = np.expand_dims(np.mean(point_set[:,0:3], axis = 0), 0) #Useless because dataset has been normalised already\n",
    "        # point_set[:,0:3] = point_set[:,0:3] - centroid\n",
    "        if not self.normal:\n",
    "            point_set = point_set[:,0:3]\n",
    "        else:\n",
    "            point_set[:,3:6] = 0.1 * point_set[:,3:6]\n",
    "        point_set = torch.from_numpy(point_set)\n",
    "\n",
    "        # load image\n",
    "        if self.SVR:\n",
    "            if self.train:\n",
    "                N_tot = len(os.listdir(fn[0])) - 3\n",
    "                if N_tot==1:\n",
    "                    print(\"only one view in \", fn)\n",
    "                if self.gen_view:\n",
    "                    N=0\n",
    "                else:\n",
    "                    N = np.random.randint(1,N_tot)\n",
    "                if N < 10:\n",
    "                    im = Image.open(os.path.join(fn[0], \"0\" + str(N) + \".png\"))\n",
    "                else:\n",
    "                    im = Image.open(os.path.join(fn[0],  str(N) + \".png\"))\n",
    "\n",
    "                im = self.dataAugmentation(im) #random crop\n",
    "            else:\n",
    "                if self.idx < 10:\n",
    "                    im = Image.open(os.path.join(fn[0], \"0\" + str(self.idx) + \".png\"))\n",
    "                else:\n",
    "                    im = Image.open(os.path.join(fn[0],  str(self.idx) + \".png\"))\n",
    "                im = self.validating(im) #center crop\n",
    "            data = self.transforms(im) #scale\n",
    "            data = data[:3,:,:]\n",
    "        else:\n",
    "            data = 0\n",
    "        return data, point_set.contiguous(), fn[2], fn[3], fn[4]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datapath)\n",
    "\n",
    "\n",
    "\n",
    "if __name__  == '__main__':\n",
    "\n",
    "    print('Testing Shapenet dataset')\n",
    "    d  =  ShapeNet(class_choice =  None, balanced= False, train=True, npoints=2500)\n",
    "    a = len(d)\n",
    "    d  =  ShapeNet(class_choice =  None, balanced= False, train=False, npoints=2500)\n",
    "    a = a + len(d)\n",
    "    print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
